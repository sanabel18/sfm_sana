import json
import numpy as np
import os
import pandas as pd
import re
import subprocess
import shutil
import trimesh
import utm
import itertools

from numpy import concatenate as concat
from os.path import join
from scipy.optimize import least_squares
from scipy.stats import zscore
from scipy.spatial.transform import Rotation as R
from task import Task
import trimesh

from utils.exportutil import export_route_obj_visualizations, export_sfm_data_visualization
from utils.genutil import safely_create_dir, transform_sfm_data, TransData, compose_trf_mat, decompose_trf_mat, \
        extend_sfm_data_with_footprints, extend_sfm_data_with_stepsize_anchor, reduce_views, \
        reorder_sfm_data
from utils.meshlabutil import GenMesh, ProcMlx
from utils.meshdata import CurvMeshData, MeshData
from utils.safe_json import safe_json_dump
from utils.segutil import seg_kmeans
from utils.transformation import cRt_to_trf_mat, apply_transform, ransac_similarity_transform
import tool.routes_to_gps as R2G 
from tool.route_filler import NpEncoder, RouteFiller

class CleanMesh(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        
        self.data_dir = prj_dirs['data']
        self.postproc_dir = prj_dirs['postproc']
        
        # Print the members of the object
        self.logger.info(f'Object: {self.__dict__}')
        
    def run_task_content(self) -> int:
        # Definition of paths (simplified mesh path in postproc dir will be generated by gen_simplified_mesh)
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        clean_mesh_name = 'clean_mesh.ply'
        final_mesh_path = join(self.data_dir, 'transformed_mesh.ply')

        # Delete small isolated meshes
        returncode, clean_mesh_path = GenMesh.del_isolated_mesh(
            input_mesh_path=orig_mesh_path, output_dir=self.postproc_dir, output_mesh_name=clean_mesh_name, logger=self.logger)
        if returncode != 0:
            self.logger.error('Failed to clean up mesh, stop mesh simplification.')
            return returncode

        shutil.copyfile(clean_mesh_path, final_mesh_path)
        self.logger.info(f'The file {final_mesh_path} is replaced by the clean version {clean_mesh_path}.')
        return returncode


class SimplifyMesh(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        
        self.n_clusters = prj_cfg['simplify']['n_clusters']
        self.normal_test_thr = prj_cfg['simplify']['normal_test_thr']
        self.bounding_box_buffer = prj_cfg['simplify']['bounding_box_buffer']
        self.seg_size_thr = prj_cfg['simplify']['seg_size_thr']
        
        self.data_dir = prj_dirs['data']
        self.postproc_dir = prj_dirs['postproc']
        self.seg_dir = prj_dirs['seg']
        # Clean up all filters in seg directory
        for file in os.listdir(self.seg_dir):
            file_path = join(self.seg_dir, file)
            if os.path.isdir(file_path):
                shutil.rmtree(file_path)
            else:
                os.remove(file_path)
        
        # Print the members of the object
        self.logger.info(f'Object: {self.__dict__}')
        
    def run_task_content(self) -> int:
        # Definition of paths (simplified mesh path in postproc dir will be generated by gen_simplified_mesh)
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        curv_mesh_name = 'coarse_mesh_with_curv.ply'
        clean_mesh_name = 'clean_mesh.ply'
        final_mesh_path = join(self.data_dir, 'transformed_mesh.ply')

        # First of all, delete small isolated meshes
        returncode, clean_mesh_path = GenMesh.del_isolated_mesh(
            input_mesh_path=orig_mesh_path, output_dir=self.postproc_dir, output_mesh_name=clean_mesh_name, logger=self.logger)
        if returncode != 0:
            self.logger.error('Failed to clean up mesh, stop mesh simplification.')
            return returncode

                
        # Generate a new mlx to reduce the size of the mesh before further calculations
        returncode, curv_mesh_path = GenMesh.gen_curv_mesh(
            input_mesh_path=clean_mesh_path, output_dir=self.postproc_dir, output_mesh_name=curv_mesh_name, logger=self.logger)
        if returncode != 0:
            self.logger.error('Failed to generate reduced mesh with curvature info, stop mesh simplification.')
            return returncode

        # Read mesh data with curvature info
        mesh_data = CurvMeshData(curv_mesh_path)

        # Run segmentation using KMeans
        seg_kmeans(
            data=mesh_data, n_clusters=self.n_clusters, normal_test_thr=self.normal_test_thr,
            bounding_box_buffer=self.bounding_box_buffer, min_seg_size=self.seg_size_thr[0],
            output_dir=self.seg_dir, logger=self.logger)
        
        # Apply segmentation results on original mesh and simplify it
        # returncode, simp_mesh_path = GenMesh.gen_simplified_mesh(
        #     mesh_path=clean_mesh_path, seg_size_thr=self.seg_size_thr,
        #     seg_dir=self.seg_dir, output_dir=self.postproc_dir, logger=self.logger)
        
        # Copy simplified mesh to data directory (as transformed mesh)
        # shutil.copyfile(simp_mesh_path, final_mesh_path)
        # self.logger.info(f'The file {final_mesh_path} is updated.')
        
        shutil.copyfile(clean_mesh_path, final_mesh_path)
        self.logger.info(f'The file {final_mesh_path} is replaced by the clean version {clean_mesh_path}.')
        self.logger.info(f'Coarse mesh is successfully generated, but the model mesh is not simplified but only cleaned up (which is what we want).')
        return returncode


class CorrectLevel(Task):
    ''' The coordinate transformation should be calculated and done here: Level correction. '''

    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])    

        self.loc_dir = prj_dirs['loc']
        self.seg_dir = prj_dirs['seg']
        self.postproc_dir = prj_dirs['postproc']
        self.data_dir = prj_dirs['data']

        self.normal_test_thr = prj_cfg['correct_lv']['normal_test_thr']
        self.sigma_fac = prj_cfg['correct_lv']['sigma_fac']
        self.correctness_thr = prj_cfg['correct_lv']['correctness_thr']
        
    def run_task_content(self) -> int:
        '''
        Workflow as follows:
            1. Extract flat faces from the group mlx files.
            2. Pick the groups that are aligned with given axis (normally y axis) and make a new mlx & a face id list with them.
            3. If step 2 succeeded, calculate a robust mean normal of these faces.
            4. Check if the mean normal is already very closed to the given axis, if no then calculate the rotation needed to make them align.
            5. Transform the model and all extrinsics.
        '''
        
        # Path preparations
        orig_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        lv_cor_sfm_data_path = join(self.postproc_dir, 'sfm_data_level_corrected.json')
        final_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        curv_mesh_path = join(self.postproc_dir, 'coarse_mesh_with_curv.ply')
        lv_cor_mesh_name = 'simplified_level_corrected_mesh.ply'
        final_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        transformation_path = join(self.data_dir, 'transformation.json')

        # Generate floor mlx and check if the mean normal of the floor aligns
        floor_fids, merged_mlx_path = ProcMlx.merge_aligned_mlx_from_segments(
            normal_test_thr=self.normal_test_thr, seg_dir=self.seg_dir, logger=self.logger)
        curv_mesh_data = CurvMeshData(curv_mesh_path)
        # _, mean_alignment = curv_mesh_data.get_mean_aligned_face_normal(fid_list=floor_fids, sigma_fac=self.sigma_fac)
        
        # Calculate level correction using coarse mesh
        returncode, lv_cor_dict = GenMesh.calc_alignment_rot(
            input_mesh_path=curv_mesh_path, select_mlx_path=merged_mlx_path, output_dir=self.postproc_dir, logger=self.logger)
        lv_cor_mat_4x4 = compose_trf_mat(lv_cor_dict['scale'], lv_cor_dict['angle_deg'], lv_cor_dict['axis'], lv_cor_dict['offset'])

        # Apply the level correction on mesh and sfm data
        returncode, trans_mesh_path = GenMesh.gen_transformed_mesh(
            input_mesh_path=orig_mesh_path, output_dir=self.postproc_dir, output_mesh_name=lv_cor_mesh_name, trans_dict=lv_cor_dict, logger=self.logger)
        transform_sfm_data(
            input_sfm_data_path=orig_sfm_data_path, output_sfm_data_path=lv_cor_sfm_data_path, trf_mat=lv_cor_mat_4x4, logger=self.logger)
        
        # Copy files to data directory
        trans_data = TransData().set_from_json(transformation_path)
        trans_data.append_trf(lv_cor_mat_4x4, note=type(self).__name__)
        trans_data.write2json(transformation_path)
        shutil.copyfile(trans_mesh_path, final_mesh_path)
        shutil.copyfile(lv_cor_sfm_data_path, final_sfm_data_path)
        self.logger.info(f'The files {final_sfm_data_path}, {final_mesh_path} & {transformation_path} are updated.')
        
        return 0

    
class GenFootprints(Task):
    ''' Generate footprints (no transformation) and save in a new sfm data json file. '''

    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        self.postproc_dir = prj_dirs['postproc']
        self.data_dir = prj_dirs['data']
            
    def run_task_content(self) -> int:
        # Definition of names & paths
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        orig_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        footprint_sfm_data_path = join(self.postproc_dir, 'sfm_data_with_footprint.json')
        final_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')

        with open(orig_sfm_data_path, 'r') as f:
            camera_dict = json.load(f)
        self.logger.info(f'Camera poses from {orig_sfm_data_path} loaded. Start calculating footprints...')
        
        # Iterate through all views to calculate footprint
        center_list = []
        direction_list = []
        for extrinsic in camera_dict['extrinsics']:
            center_list.append(extrinsic['value']['center'])
            direction_list.append(np.array(extrinsic['value']['rotation'])[1, :])
        center_list = np.array(center_list)
        direction = np.mean(direction_list, axis=0)

        mesh = trimesh.load(orig_mesh_path)
        locations, index_ray_list, _ = mesh.ray.intersects_location(
                                    ray_origins=center_list,
                                    ray_directions=[direction]*len(center_list),
                                    multiple_hits=False)

        footprint_list = [None] * len(center_list)
        if len(locations) > 0:
            distance_list= np.linalg.norm(np.array(center_list)[index_ray_list] - locations, axis=1)
            min_dist = np.percentile(distance_list, 30, interpolation='nearest')
            median_dist = np.percentile(distance_list, 50, interpolation='nearest')
            max_dist = np.percentile(distance_list, 70, interpolation='nearest')
            
            for i, location, distance in zip(index_ray_list, locations, distance_list):
                if distance < min_dist:
                    location = center_list[i] + min_dist * direction
                elif distance > max_dist:
                    location = center_list[i] + max_dist * direction
                footprint_list[i] = location

            for i in range(len(footprint_list)):
                if footprint_list[i] is None:
                    footprint_list[i] = center_list[i] + median_dist * direction
        else:
            self.logger.warn('Bad Model Cause Bad Footprint')
            for i in range(len(footprint_list)):
                if footprint_list[i] is None:
                    footprint_list[i] = center_list[i]

        for extrinsic, footprint in zip(camera_dict['extrinsics'], footprint_list):
            extrinsic['value']['footprint'] = footprint.tolist()

        self.logger.info('Footprints calculation done.')
        
        # Write footprints back to the original sfm data file
        with open(footprint_sfm_data_path, 'w') as file:
            safe_json_dump(camera_dict, file)
        self.logger.info(f'Footprints successfully written to the sfm data file {footprint_sfm_data_path}.')
        
        # Copy file to data directory
        shutil.copyfile(footprint_sfm_data_path, final_sfm_data_path)
        self.logger.info(f'The file {final_sfm_data_path} is updated.')
        
        return 0
    

class GenFootprintsByTrimesh(Task):
    ''' Generate footprints (no transformation) and save in a new sfm data json file. '''

    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        self.postproc_dir = prj_dirs['postproc']
        self.data_dir = prj_dirs['data']
            
    def run_task_content(self) -> int:
        # Definition of names & paths
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        orig_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        footprint_sfm_data_path = join(self.postproc_dir, 'sfm_data_with_footprint.json')
        final_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        
        camera_dict = extend_sfm_data_with_footprints(
            orig_sfm_data_path, orig_mesh_path, self.logger)

        # Write footprints back to the original sfm data file
        with open(footprint_sfm_data_path, 'w') as file:
            safe_json_dump(camera_dict, file)
        self.logger.info(f'Footprints successfully written to the sfm data file {footprint_sfm_data_path}.')
        
        # Copy file to data directory
        shutil.copyfile(footprint_sfm_data_path, final_sfm_data_path)
        self.logger.info(f'The file {final_sfm_data_path} is updated.')
        
        return 0

class GenStepSizeAnchor(Task):
    """
    from sfm_data_roginal.json, use its camera poses to calculate step-size anchor
    this task should be included in recon project if opt_with_stepsize_anchor was enabled in loc project.
    """
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        self.postproc_dir = prj_dirs['postproc']
        self.data_dir = prj_dirs['data']
            
    def run_task_content(self) -> int:
        # Definition of names & paths
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        orig_sfm_data_path = join(self.data_dir, 'sfm_data_original.json')
        stepsize_anchor_sfm_data_path = join(self.postproc_dir, 'sfm_data_with_stepsize_anchor.json')
        orig_sfm_data_stepsize_anchor_path = join(self.data_dir, 'sfm_data_stepsize_anchor_original.json')
        final_sfm_data_stepsize_anchor_path = join(self.data_dir, 'sfm_data_stepsize_anchor_transformed.json')
        
        camera_dict = extend_sfm_data_with_stepsize_anchor(
            orig_sfm_data_path, self.logger)

        # Write footprints back to the original sfm data file
        with open(stepsize_anchor_sfm_data_path, 'w') as file:
            safe_json_dump(camera_dict, file)
        self.logger.info(f'Footprints successfully written to the sfm data file {stepsize_anchor_sfm_data_path}.')
        
        # Copy file to data directory
        shutil.copyfile(stepsize_anchor_sfm_data_path, final_sfm_data_stepsize_anchor_path)
        shutil.copyfile(final_sfm_data_stepsize_anchor_path, \
                        orig_sfm_data_stepsize_anchor_path)  # Make a copy, localization project (MatchModels) will need it.
        self.logger.info(f'The file {final_sfm_data_stepsize_anchor_path} is updated.')
        
        return 0



class ZeroInitPose(Task):
    '''
    Transform both sfm data and mesh so that the 1st frame has no rotation at all.
    This is needed for user to have a natural view at the beginning of the video.
    '''
    
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])
        self.postproc_dir = prj_dirs['postproc']
        self.data_dir = prj_dirs['data']
            
    def run_task_content(self) -> int:
        # Definition of names & paths
        orig_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        zeroed_mesh_name = 'zeroed_mesh.ply'
        final_mesh_path = join(self.data_dir, 'transformed_mesh.ply')
        orig_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        zeroed_sfm_data_path = join(self.postproc_dir, 'sfm_data_zeroed.json')
        final_sfm_data_path = join(self.data_dir, 'sfm_data_transformed.json')
        transformation_path = join(self.data_dir, 'transformation.json')
        
        orig_sfm_data_stepsize_anchor_path = join(self.data_dir, 'sfm_data_stepsize_anchor_transformed.json')
        zeroed_sfm_data_stepsize_anchor_path = join(self.postproc_dir, 'sfm_data_step_size_anchor_zeroed.json')
        final_sfm_data_stepsize_anchor_path = join(self.data_dir, 'sfm_data_stepsize_anchor_transformed.json')
 
        with open(orig_sfm_data_path, 'r') as f:
            camera_dict = json.load(f)
        self.logger.info(f'Camera poses & footprints from {orig_sfm_data_path} loaded. Start zeroing initial pose...')       
        
        # Calculate the transformation of initial pose zeroing
        rot_pose0_matrix = np.array(camera_dict['extrinsics'][0]['value']['rotation'])
        rot_pose0_vec = R.from_matrix(rot_pose0_matrix).as_rotvec()
        rot_pose0_angle_rad = np.linalg.norm(rot_pose0_vec)
        rot_pose0_axis = rot_pose0_vec / rot_pose0_angle_rad
        rot_pose0_dict = {'scale': 1, 'angle_deg': np.rad2deg(rot_pose0_angle_rad), 'axis': rot_pose0_axis.tolist(), 'offset': [0, 0, 0]}
        rot_pose0_mat_4x4 = compose_trf_mat(rot_pose0_dict['scale'], rot_pose0_dict['angle_deg'], rot_pose0_dict['axis'], rot_pose0_dict['offset'])

        # Do initial pose zeroing on both sfm data file and mesh 
        transform_sfm_data(
            input_sfm_data_path=orig_sfm_data_path, \
            output_sfm_data_path=zeroed_sfm_data_path, \
            trf_mat=rot_pose0_mat_4x4, logger=self.logger)
        if os.path.isfile(orig_sfm_data_stepsize_anchor_path):
            self.logger.info('file exist {orig_sfm_data_stepsize_anchor_path}, do transformation')
            transform_sfm_data(
                input_sfm_data_path=orig_sfm_data_stepsize_anchor_path, \
                output_sfm_data_path=zeroed_sfm_data_stepsize_anchor_path, \
                trf_mat=rot_pose0_mat_4x4, logger=self.logger)
        
        if os.path.isfile(orig_mesh_path):
            returncode, zeroed_mesh_path = GenMesh.gen_transformed_mesh(
                        input_mesh_path=orig_mesh_path, \
                        output_dir=self.postproc_dir, \
                        output_mesh_name=zeroed_mesh_name, \
                        trans_dict=rot_pose0_dict, logger=self.logger)
        else:
            zeroed_mesh_path = join(self.data_dir, zeroed_mesh_name)
        # Copy file to data directory
        trans_data = TransData().set_from_json(transformation_path)
        trans_data.append_trf(rot_pose0_mat_4x4, note=type(self).__name__)
        trans_data.write2json(transformation_path)
        shutil.copyfile(zeroed_sfm_data_path, final_sfm_data_path)
        if os.path.isfile(zeroed_sfm_data_stepsize_anchor_path):
            shutil.copyfile(zeroed_sfm_data_stepsize_anchor_path, \
                    final_sfm_data_stepsize_anchor_path)
        if os.path.isfile(zeroed_mesh_path):
            shutil.copyfile(zeroed_mesh_path, final_mesh_path)
            self.logger.info(f'The file {final_mesh_path} is updated')
        self.logger.info(f'The file {final_sfm_data_path} is  updated.')
        
        return 0
        
        
class ExportModel2App(Task):
    ''' Generate 2 things that the app needs: route file & obj-format mesh. '''
    
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])

        self.data_dir = prj_dirs['data']
        self.img_prj_dir = prj_dirs['img_prj']
        self.n_lense_for_rot = prj_cfg['project']['n_lense_for_rot']

    def run_task_content(self) -> int:
        sfm_data_stepsize_path = os.path.join(self.data_dir, 'sfm_data_stepsize_anchor_transformed.json')
        if os.path.isfile(sfm_data_stepsize_path):
            export_sfm_data_visualization(sfm_data_stepsize_path, self.logger)
        return export_route_obj_visualizations(
                    self.data_dir, \
                    join(self.img_prj_dir, 'routes.json'), \
                    self.n_lense_for_rot, \
                    self.logger)
    
class FillRoute(Task):
    ''' Generate 2 things that the app needs: route file & obj-format mesh. '''
    
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
        
        super().__init__(step, prj_dirs['log'])

        self.data_dir = prj_dirs['data']
        self.routes_file = join(self.data_dir, 'routes.json')
        self.filled_routes_file = join(self.data_dir, 'filled_routes.json')

    def run_task_content(self) -> int:
        route_filler = RouteFiller(self.routes_file, self.logger)
        new_route = route_filler.fill_route()
        json_object = json.dumps(new_route, indent=4, cls=NpEncoder)
        with open(self.filled_routes_file, "w") as outfile:
            outfile.write(json_object)
            outfile.close()
        if new_route:
            return 0 
   
class CalcTransSub2Main(Task):
    ''' 
    Calculate the transformation from sub to main coordinate with the results of localization command (i.e. MatchModels). 
    Here we have two sfm models that actually represent same pysical positions(route in subscene) :
    subscene_json_path: poses of route in subscene in subscene coord. system
    sub2main_json_path: poses of route in subscene in mainscene coord. system
    find the transformation from subscene_json_path to sub2main_json_path, then we 
    know how to transform all points from subscene coord. system into mainscene coord. system
    if with mode opt_with_stepsize_anchor = True, the footprints generated as 
    stepsize anchor is strongly depend on the step size between camera poses, therefore the
    consistency between camera poses between two sfm models is importand.
    in that case, reduce_views make sure the views in subscene is reduced to only views
    appearing in sub2main_json_path

    '''
    
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
    
        super().__init__(step, prj_dirs['log'])

        self.main_data_dir = prj_dirs['main_data']
        self.sub_data_dir = prj_dirs['sub_data']
        self.loc_dir = prj_dirs['loc']
        
        self.opt_with_footprints = prj_cfg['transform']['opt_with_footprints'] \
            if 'opt_with_footprints' in prj_cfg['transform'] else False
        self.opt_with_stepsize_anchor = prj_cfg['transform']['opt_with_stepsize_anchor'] \
            if 'opt_with_stepsize_anchor' in prj_cfg['transform'] else False
        
 
        return
            
    def run_task_content(self) -> int:
        # Definition of names & paths
        main_orig_mesh_path = join(self.main_data_dir, 'original_mesh.ply')
        sub_orig_mesh_path = join(self.sub_data_dir, 'original_mesh.ply')
        sub2main_json_path = join(self.loc_dir, 'sfm_data_sub2main.json')
        subscene_json_path = join(self.loc_dir, 'sfm_data_original_sub.json')
        subscene_reduced_json_path = join(self.loc_dir, 'sfm_data_original_sub_reduced.json')
        subscene_w_footprint_json_path = join(self.loc_dir, 'sfm_data_original_sub_wfp.json')
        sub2main_w_footprint_json_path = join(self.loc_dir, 'sfm_data_sub2main_wfp.json')
        loc_trans_json_path = join(self.loc_dir, 'loc_trans_sub2main.json')
            
        # Preparation
        if self.opt_with_stepsize_anchor:
            # reduce views from subscene json, to be consistant with the one in sub2main
            reduce_views(sub2main_json_path, subscene_json_path, subscene_reduced_json_path, self.logger)
            sub2main_dict = extend_sfm_data_with_stepsize_anchor(sub2main_json_path, self.logger)
            subscene_dict = extend_sfm_data_with_stepsize_anchor(subscene_reduced_json_path, self.logger)
            safe_json_dump(sub2main_dict, open(sub2main_w_footprint_json_path,'w'))
            safe_json_dump(subscene_dict, open(subscene_w_footprint_json_path,'w'))
            # export visualization 
            export_sfm_data_visualization(sub2main_w_footprint_json_path, self.logger)
            export_sfm_data_visualization(subscene_w_footprint_json_path, self.logger)
            footprint_main = []
            footprint_sub = [] 
        elif self.opt_with_footprints:
            sub2main_dict = extend_sfm_data_with_footprints(sub2main_json_path, main_orig_mesh_path, self.logger)
            subscene_dict = extend_sfm_data_with_footprints(subscene_json_path, sub_orig_mesh_path, self.logger)
            # export visualization 
            export_sfm_data_visualization(sub2main_w_footprint_json_path, self.logger)
            export_sfm_data_visualization(subscene_w_footprint_json_path, self.logger)
            footprint_main = []
            footprint_sub = []
        else:
            with open(sub2main_json_path) as f:
                sub2main_dict = json.load(f)
            with open(subscene_json_path) as f:
                subscene_dict = json.load(f)
        img_pair_info = []  # For calc_trf_sub2main
        x_main = []  # For calc_avg_sqdist
        r_main = []  # For calc_avg_sqdist
        x_sub = []  # For calc_avg_sqdist
        r_sub = []  # For calc_avg_sqdist
        self.logger.info('Start finding and pairing the extrinsics of views in the two models...')
        
        # Find and pair the extrinsics of views in the two models
        max_key = -1
        for extr in sub2main_dict['extrinsics']:
            max_key = max(max_key, extr['key'])
         
        for extr in sub2main_dict['extrinsics']:
            if self.opt_with_stepsize_anchor and extr['key'] == max_key:
                continue
            # 1. In sub2main: From extrinsics key, find image filename
            for idx, view in enumerate(sub2main_dict['views']):
                if extr['key']==view['key']:
                    img_pair_info.append({
                        'sub2main_filename': sub2main_dict['views'][idx]['value']['ptr_wrapper']['data']['filename'],
                        'sub2main_extr': extr['value']})
                    break
            else:
                self.logger.warn(f"No corresponding view found in sub2main sfm data for extrinsics w/ key {extr['key']}")
                break
                
            # 2. In subscene: From sub2main image filename, find image key and then find extrinsics using key
            # '0001.jpg' -> ['0001', 'jpg'] -> '0001'
            # 'blabla.jpg' -> ['blabla', 'jpg'] -> 'blabla'
            img_nr_str = re.split('\.', img_pair_info[-1]['sub2main_filename'])[0]
            for view in subscene_dict['views']:
                if img_nr_str == view['value']['ptr_wrapper']['data']['filename'].split('.')[0]:
                    # Look for the corresponding extrinsics w/ the key of the view
                    for idx, extr in enumerate(subscene_dict['extrinsics']):
                        if extr['key'] == view['key']:                    
                            img_pair_info[-1]['subscene_filename'] = view['value']['ptr_wrapper']['data']['filename']
                            img_pair_info[-1]['subscene_extr'] = subscene_dict['extrinsics'][idx]['value']
                            self.logger.info(f'A new pair is found: {img_pair_info[-1]}\n')
                            x_main.append(img_pair_info[-1]['sub2main_extr']['center'])
                            r_main.append(img_pair_info[-1]['sub2main_extr']['rotation'])
                            x_sub.append(img_pair_info[-1]['subscene_extr']['center'])
                            if self.opt_with_footprints or self.opt_with_stepsize_anchor:
                                footprint_main.append(img_pair_info[-1]['sub2main_extr']['footprint'])
                                footprint_sub.append(img_pair_info[-1]['subscene_extr']['footprint'])
                            break
                    else:
                        self.logger.warn(f"No corresponding extrinsics found in sub sfm data for image {view['value']['ptr_wrapper']['data']['filename']}")
                    break  # No matter if the corresponding extrinsics is found, break the outer for loop
            if not 'subscene_filename' in img_pair_info[-1]:
                self.logger.warn(f'This image did not form a pair, discarded: {img_pair_info.pop()}\n')
        self.logger.info(f'Total number of pairs found = {len(img_pair_info)}')

        # x should be (4, n)-dimensional array, compatible with 4x4 transformation matrix
        dim = (len(x_main), 1)
        x_main = concat((np.array(x_main), np.ones(dim)), axis=1).transpose()
        x_sub = concat((np.array(x_sub), np.ones(dim)), axis=1).transpose()
        if self.opt_with_footprints or self.opt_with_stepsize_anchor:
            footprint_main = concat((np.array(footprint_main), np.ones(dim)), axis=1).transpose()
            footprint_sub = concat((np.array(footprint_sub), np.ones(dim)), axis=1).transpose()
                            
        # Pick reliable images and use them to do least square
        reliable_imgs, avg_scale, trf_mat_pick = self.pick_reliable_imgs(dim[0], img_pair_info)
        if reliable_imgs is None:
            if self.opt_with_footprints or self.opt_with_stepsize_anchor:
                concated_main = concat((x_main, footprint_main), axis=1)
                concated_sub = concat((x_sub, footprint_sub), axis=1)
                trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(
                    concated_main, concated_sub)
                #trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(footprint_main, footprint_sub)
                pass
            else:
                trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(x_main, x_sub)
        else:
            if self.opt_with_footprints or self.opt_with_stepsize_anchor:
                concated_main = concat((x_main[:, reliable_imgs], footprint_main[:, reliable_imgs]), axis=1)
                concated_sub = concat((x_sub[:, reliable_imgs], footprint_sub[:, reliable_imgs]), axis=1)
                trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(
                    concat((x_main[:, reliable_imgs], footprint_main[:, reliable_imgs]), axis=1),
                    concat((x_sub[:, reliable_imgs], footprint_sub[:, reliable_imgs]), axis=1))
                # trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(footprint_main[:, reliable_imgs], footprint_sub[:, reliable_imgs])
            else:
                trf_mat_opt, sol_opt = self.pos_opt_trf_sub2main(x_main[:, reliable_imgs], x_sub[:, reliable_imgs])
        self.logger.info(f'Tranformation matrix: \nFrom candidate pair: \n{np.array(trf_mat_pick)}\nFrom optimization: \n{np.array(trf_mat_opt)}')
        
        # Check if the optimized solution is close enough to the averaged one
        thr_similarity = 0.95
        if avg_scale is None:
            self.logger.warn('No similarity can be calculated since picking reliable images function failed.')
        else:
            similarity = np.minimum(np.abs(sol_opt[0]), np.abs(avg_scale)) / np.maximum(np.abs(sol_opt[0]), np.abs(avg_scale))
            self.logger.info(f'Similarity between average of reliable scale & optimization solution = {similarity}.')
            if similarity > thr_similarity:
                self.logger.info(f'Similarity is larger than {thr_similarity}, passed.')
            else:
                self.logger.warning(f'Similarity is NOT larger than {thr_similarity}, something must be wrong.')
        
        # Write to file
        with open(loc_trans_json_path, 'w') as f:
            trf_mat_final = trf_mat_opt
            self.logger.info(f'Final transformation used: {decompose_trf_mat(trf_mat_final)}')
            safe_json_dump(trf_mat_final, f)
        self.logger.info(f'Saved transformation matrix as {loc_trans_json_path}.')
        
        return 0    

    def calc_avg_sqdist(self, trf, x_main, x_sub):
        ''' Calculate the average squared distances. Input transformation is numpy array of [scale, quaternion, offset].'''
        
        rotvec = R.from_quat(trf[1:5]).as_rotvec()
        trf_4x4 = compose_trf_mat(
            trf[0], np.linalg.norm(rotvec), rotvec / np.linalg.norm(rotvec), trf[5:])
        x_sub2main = np.matmul(trf_4x4, x_sub)
        deltas = np.sqrt(np.sum(np.square(x_main - x_sub2main), axis=None)) / x_main.shape[1]
        print(deltas)
        return deltas
    
    def similarity_transform(self, from_pts, to_pts, is_colvec=True):
        '''
        Umeyama tranform: https://pdfs.semanticscholar.org/d107/231cce2676dbeea87e00bb0c587c280b9c53.pdf?_ga=2.96067861.842228479.1608107486-40852687.1608107486
        Code adapted from: https://gist.github.com/dboyliao/f7f862172ed811032ba7cc368701b1e8
        
        Input: x_sub=from_pts, x_main=to_pts (3-dimensional, not 4!!!)
        Output: Similarity transformation (scale, rotation matrix, offset)
        '''
        np.save(join(self.loc_dir, 'sub.npy'),from_pts)
        np.save(join(self.loc_dir, 'main.npy'),to_pts)
        if is_colvec:
            from_points = from_pts.T
            to_points = to_pts.T
        else:
            from_points = from_pts
            to_points = to_pts

        assert len(from_points.shape) == 2, \
            "from_points must be a m x n array"
        assert from_points.shape == to_points.shape, \
            "from_points and to_points must have the same shape"

        N, m = from_points.shape

        mean_from = from_points.mean(axis = 0)
        mean_to = to_points.mean(axis = 0)

        delta_from = from_points - mean_from # N x m
        delta_to = to_points - mean_to       # N x m

        sigma_from = (delta_from * delta_from).sum(axis = 1).mean()
        sigma_to = (delta_to * delta_to).sum(axis = 1).mean()

        cov_matrix = delta_to.T.dot(delta_from) / N

        U, d, V_t = np.linalg.svd(cov_matrix, full_matrices = True)
        cov_rank = np.linalg.matrix_rank(cov_matrix)
        S = np.eye(m)

        if cov_rank >= m - 1 and np.linalg.det(cov_matrix) < 0:
            S[m-1, m-1] = -1
        elif cov_rank < m-1:
            raise ValueError("colinearility detected in covariance matrix:\n{}".format(cov_matrix))

        R = U.dot(S).dot(V_t)
        c = (d * S.diagonal()).sum() / sigma_from
        t = mean_to - c * R.dot(mean_from)

        return c, R, t

    
    def pos_opt_trf_sub2main(self, x_main: np.ndarray, x_sub: np.ndarray):        
        scale, rot_mat, offset = self.similarity_transform(x_sub[:3,:], x_main[:3,:])
        trf_mat_sol = np.concatenate((np.concatenate((scale * rot_mat, offset.reshape((-1,1))), axis=1), np.array([[0,0,0,1]])), axis=0)
        result = [scale, rot_mat, offset]
        
        np.set_printoptions(linewidth=1000)
        # self.logger.info(f'Least square solution: \n{res}')
        self.logger.info(f'Transformation matrix found by optimization = {trf_mat_sol}')
        self.logger.info(f'Transformation data found by least square = {decompose_trf_mat(trf_mat_sol)}')
        return trf_mat_sol, result
    
    def calc_trf_sub2main(self, pair_row: dict, pair_col: dict) -> dict:
        # Calculate scale
        dist_sub = np.linalg.norm(np.array(pair_row['subscene_extr']['center']) - np.array(pair_col['subscene_extr']['center']))
        dist_main = np.linalg.norm(np.array(pair_row['sub2main_extr']['center']) - np.array(pair_col['sub2main_extr']['center']))
        scale = dist_main / dist_sub
        
        # Calculate rotation
        rot_sub = np.array(pair_row['subscene_extr']['rotation'])
        rot_main = np.array(pair_row['sub2main_extr']['rotation'])
        rot_matrix = np.matmul(rot_main.transpose(), rot_sub)
        rot_obj = R.from_matrix(rot_matrix)
        rot_vec = rot_obj.as_rotvec()
        rot_axis = rot_vec / np.linalg.norm(rot_vec)
        rot_angle_deg = np.rad2deg(np.linalg.norm(rot_vec))
        
        # Calculate offset
        offset = np.array(pair_row['sub2main_extr']['center']) - scale * np.matmul(rot_matrix, np.array(pair_row['subscene_extr']['center']))
        
        return compose_trf_mat(scale, rot_angle_deg, rot_axis, offset)
        
    def pick_reliable_imgs(self, dim, img_pair_info):
        ''' Calculate all scales, remove outliers strictly, then see which images are still not thrown away. '''
        
        scales = np.ma.array(np.zeros((dim, dim-1)))
        for row in range(dim):
            for col in range(dim):
                if row > col:
                    trf = self.calc_trf_sub2main(img_pair_info[row], img_pair_info[col])
                    scales[row, col] = decompose_trf_mat(trf)['scale']
                    scales[col, row-1] = scales[row, col]  # Shift the upper triangle 1 position to the left
        np.set_printoptions(linewidth=1000)        
        self.logger.info(f'scale matrix = \n{scales}')

        # Mask everything outside 1 standard deviation from median, repeat until criteria are met or loop ends
        n_loop = 10
        thr_masked = 0.7
        thr_rel_stddev = 0.05
        for iter in range(n_loop):
            median = np.percentile(scales[~scales.mask], 50, interpolation='nearest')  # np.median seems to ignore the mask, so mask scales before calling np.median
            stddev = np.std(scales)
            mask = np.logical_or((scales > median + stddev), (scales < median - stddev))
            n_masked = np.count_nonzero(mask)
            scales.mask = mask
            self.logger.info(f'------------ RUN {iter} ------------')
            self.logger.info(f'median = {median} / stddev = {stddev} / # of masked elements = {n_masked}')
            p_masked = n_masked / (dim * (dim - 1))
            rel_stddev = stddev / median
            if p_masked > thr_masked and rel_stddev < thr_rel_stddev:
                reliable_imgs = np.unique(np.argwhere(scales)[:,0])
                avg_scale = scales.mean()
                self.logger.info(f'Masked scale matrix = \n{scales}')
                self.logger.info(f'Reliable images: {reliable_imgs} / average scale = {avg_scale}')
                self.logger.info(f'Number of reliable images = {len(reliable_imgs)}')
                self.logger.info(f'Masked entries = {p_masked} > {thr_masked} and relative standard deviation = {rel_stddev} < {thr_rel_stddev}, break the loop.')

                arg = np.argwhere(scales == median)
                candidate_pair = arg[1]  # This one is row > col, so no need to "shift back"
                self.logger.info(f'Median = {scales[candidate_pair[0]][candidate_pair[1]]} @ {candidate_pair}')
                
                trf_mat_pick = self.calc_trf_sub2main(img_pair_info[candidate_pair[0]], img_pair_info[candidate_pair[1]])
                return reliable_imgs, avg_scale, trf_mat_pick
        
        self.logger.info(f'Masked scale matrix = \n{scales}')
        self.logger.warn(f'Masked entries = {p_masked} and relative standard deviation = {rel_stddev} after {n_loop} iterations. Something must be wrong.')
        return None, None, None

        
class ApplyTrfExportSubs(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
    
        super().__init__(step, prj_dirs['log'])

        self.prj_dir = prj_dirs['prj']
        self.main_prj_dir = prj_dirs['main_prj']
        self.sub_prj_dir = prj_dirs['sub_prj']
        self.main_data_dir = prj_dirs['main_data']
        self.sub_data_dir = prj_dirs['sub_data']
        self.loc_dir = prj_dirs['loc']
        
        self.type_main_prj = prj_cfg['transform']['type_main_prj']
        self.type_sub_prj = prj_cfg['transform']['type_sub_prj']
        self.overwrite_subs = prj_cfg['apply']['overwrite_subs']
        self.n_lense_for_rot = prj_cfg['project']['n_lense_for_rot']
        self.opt_with_stepsize_anchor = prj_cfg['transform']['opt_with_stepsize_anchor']

        if not 'force_copy_data_dir_main' in prj_cfg['project']:
            self.force_copy_data_dir_main = False
        else:
            self.force_copy_data_dir_main = prj_cfg['project']['force_copy_data_dir_main']
        
        return
        
    def run_task_content(self) -> int:
        # Definition of names & paths
        main_trans_json_path = join(self.main_data_dir, 'transformation.json')
        sub_trans_json_path = join(self.sub_data_dir, 'transformation.json')
        loc_trans_json_path = join(self.loc_dir, 'loc_trans_sub2main.json')
        
        # Load files
        trf_main = np.array(TransData().set_from_json(main_trans_json_path).concat_trf_mat)
        trf_sub = np.array(TransData().set_from_json(sub_trans_json_path).concat_trf_mat)
        with open(loc_trans_json_path) as f:
            trf_loc = np.array(json.load(f))
        
        # Calculate the transformation
        delta_trf_mat = np.matmul(trf_main, np.matmul(trf_loc, np.linalg.inv(trf_sub)))
        delta_trf_dict = decompose_trf_mat(delta_trf_mat.tolist())
        
        # Check if there are already sub partial projects in destination folder
        # Two reactions to choose in config: Delete and create, or just abort
        for folder in os.listdir(self.sub_prj_dir):
            dst = join(self.prj_dir, folder)
            if os.path.isdir(dst) and (folder[:5] == 'data_'):  # Check only data directories
                if self.overwrite_subs:
                    self.logger.warn(f'The destination of sub partial project {dst} already exists and will be deleted.')
                    shutil.rmtree(dst)
                else:
                    err = f'The destination of sub partial project {dst} already exists. Aborted.'
                    self.logger.error(err)
                    raise RuntimeError(err)

        # Copy all sub data directories to project (matches folders only symlink)
        sub_data_dirs = []
        for folder in os.listdir(self.sub_prj_dir):
            self.safe_copy_data_dir(folder, self.sub_prj_dir, sub_data_dirs)
        self.logger.info(f'The list of all sub data dirs that are to be transformed: {sub_data_dirs}')
        
        # If main project is reconstruction project, also copy its data directory
        if (self.type_main_prj == 'ReconProj') or self.force_copy_data_dir_main:
            for folder in os.listdir(self.main_prj_dir):
                self.safe_copy_data_dir(folder, self.main_prj_dir)
                
        # Apply transformation to all transformation json files, sfm data files, and mesh files in sub data directories
        # Also export obj and route files
        for folder_path in sub_data_dirs:
            # Paths & names
            trans_json_path = join(folder_path, 'transformation.json')
            sfm_data_path = join(folder_path, 'sfm_data_transformed.json')
            mesh_name = 'transformed_mesh.ply'
            sfm_data_stepsize_anchor_path = join(folder_path, 'sfm_data_stepsize_anchor_transformed.json')
            sfm_data_stepsize_anchor_trf_path = join(folder_path, 'sfm_data_stepsize_anchor_transformed.json')

            # Transformation
            trans_data = TransData().set_from_json(trans_json_path)
            trans_data.append_trf(delta_trf_mat.tolist(), note=type(self).__name__)
            trans_data.write2json(trans_json_path)            
            # Sfm data
            transform_sfm_data(
                        input_sfm_data_path=sfm_data_path, \
                        output_sfm_data_path=sfm_data_path, \
                        trf_mat=delta_trf_mat, logger=self.logger)
            
            if self.opt_with_stepsize_anchor:
                transform_sfm_data(
                        input_sfm_data_path=sfm_data_stepsize_anchor_path, \
                        output_sfm_data_path=sfm_data_stepsize_anchor_trf_path, \
                        trf_mat=delta_trf_mat, logger=self.logger)
 
            # Mesh
            if os.path.isfile(join(folder_path, mesh_name)):
                returncode, zeroed_mesh_path = GenMesh.gen_transformed_mesh(
                    input_mesh_path=join(folder_path, mesh_name), output_dir=folder_path, output_mesh_name=mesh_name, trans_dict=delta_trf_dict, logger=self.logger)
                if returncode !=0:
                    self.logger.error(f'Mesh transformation ends with return code {returncode} != 0. Abort.')
                    raise RuntimeError
                self.logger.info(f'Applied transformation on transformation, sfm data & mesh files in {folder_path}.')
        
            # Obj & route files
            returncode = export_route_obj_visualizations(
                folder_path, join(join(folder_path, 'img_prj'), 'routes.json'), self.n_lense_for_rot, self.logger)
            route_file = join(folder_path, "routes.json") 
            route_filler = RouteFiller(route_file, self.logger)
            new_route = route_filler.fill_route()
            if new_route:
                returncode = 0
                json_object = json.dumps(new_route, indent=4, cls=NpEncoder)
                with open(route_file, "w") as outfile:
                    outfile.write(json_object)
            else:
                returncode = -1

            if self.opt_with_stepsize_anchor:
                export_sfm_data_visualization(sfm_data_stepsize_anchor_trf_path, self.logger)
      
        return returncode

    def safe_copy_data_dir(self, folder: str, src_prj_dir: str, data_dir_list=[]):
        ''' Copy data directory in main/sub to project directory. Always make symbolic link for images and matches. '''
        
        src = join(src_prj_dir, folder)
        if os.path.isdir(src) and (folder[:5] == 'data_'):
            dst = safely_create_dir(self.prj_dir, folder)
            for item in os.listdir(src):
                if (item == 'img_prj') or (item == 'matches'):
                    self.logger.info(f'Making symbolic link for {item} from {src} to {dst}...')
                    os.symlink(join(src, item), join(dst, item))  # To save space
                elif item == '.ipynb_checkpoints':
                    self.logger.warn(f'Ignored {item}.')
                else:
                    try:
                        self.logger.info(f'Copying {item} from {src} to {dst}...')
                        shutil.copy(join(src, item), join(dst, item))  # Except for img and matches, everything is file not folder
                    except:
                        self.logger.warn(f'Unexpected folder {item} is in sub data directory. Ignored.')
            data_dir_list.append(dst)
            self.logger.info(f'All copying and symbolic linking done for {folder}.')
        else:
            self.logger.warn(f'Ignored {folder}.')


class Copy2Tower(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
    
        super().__init__(step, prj_dirs['log'])

        return
        
    def run_task_content(self) -> int:
        self.logger.warn('Copy to Tower is now done at superprj level. This task is deprecated and does nothing. It will be removed after 2021/1/8...')
        return 0


class ConcatConvert2Geojson(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
    
        super().__init__(step, prj_dirs['log'])

        self.prj_dir = prj_dirs['prj']

        if ('remake_routes' in prj_cfg['geojson']):
            self.remake_routes = prj_cfg['geojson']['remake_routes']
            if self.remake_routes:
                self.n_lense_for_rot = prj_cfg['project']['n_lense_for_rot']
        else:
            self.remake_routes = False
        
        self.gps = prj_cfg['geojson']['gps']
        self.world_clock_time = prj_cfg['geojson']['world_clock_time']
        self.rotation_deg = prj_cfg['geojson']['rotation_deg']
        self.scale = prj_cfg['geojson']['scale']
        self.offset = prj_cfg['geojson']['offset']
 
        
        return
        
    def run_task_content(self) -> int:
        
        # 1. Gather all information in all routes.json files
        routes = {}  # {'video_name_1':{'1':[], '2':[], '3':[], ...}, 'video_name_2': ...}

        for data_dir in [folder for folder in os.listdir(self.prj_dir)
                         if (folder[:5]=='data_') and (os.path.isdir(join(self.prj_dir, folder)))]:
            
            # Optional: Remake all routes.json
            if self.remake_routes:
                returncode = export_route_obj_visualizations(
                    join(self.prj_dir, data_dir), join(self.prj_dir, data_dir, 'routes.json'),
                    self.n_lense_for_rot, self.logger) 
            
            with open(join(self.prj_dir, data_dir, 'routes.json'), 'r') as f:
                route_dict = json.load(f)
            
            if not route_dict['video'] in routes:
                routes[route_dict['video']] = {}
            routes[route_dict['video']][route_dict['frames'][0]['slice']] = route_dict['frames']

        # 2. Pick, rearrange & transform information that frontend really needs (filename, timestamp, xyz, rot) per video
        for video_path in routes:
            geojson = {'type': 'Feature',
                       'geometry': {'type': 'LineString', 'coordinates': []},
                       'properties': {'video': video_path}}
            
            for slice_id in sorted(routes[video_path]):
                slice_list = routes[video_path][slice_id]
                self.logger.info(f'Now parsing the slice {slice_id} of video {video_path}...')
                
                # Rearrange & transform per frame
                for frame in slice_list:
                    manual_transformed_frame = self.manual_trf_per_frame(frame, self.scale, self.rotation_deg, self.offset)
                    babylon_euler_angles = R2G.trf_orientation_2_babylon(manual_transformed_frame['rotation'])
                    height = np.linalg.norm(
                        np.array(manual_transformed_frame['footprint']) - np.array(manual_transformed_frame['position']))
                    geodata = self.make_geodata(
                        frame['timestamp'], manual_transformed_frame['position'],
                        babylon_euler_angles, height,
                        self.gps, self.world_clock_time)
 
                    geojson['geometry']['coordinates'].append(geodata)

            # Save both locally and in repo
            video_folder, video_name = os.path.split(video_path)
            video_folder_name = os.path.basename(video_folder)
            geojson_path_local = join(self.prj_dir, video_folder_name + '.geojson')
            geojson_path_repo = join(video_folder, 'geo_data_sfm.geojson')
            with open(geojson_path_local, 'w') as f:
                json.dump(geojson, f)
            with open(geojson_path_repo, 'w') as f:
                json.dump(geojson, f)
            
            self.logger.info(f'The geojson for {video_name} is generated and saved as: {geojson_path_local} & {geojson_path_repo}')
        
        return 0
    
    def manual_trf_per_frame(self, frame_info: dict, scale, rot_deg, offset) -> dict:
        ''' Apply the manual transformation (scale, rotation, offset) in OpenMVG coordinate system. '''
        
        rot_rad = np.deg2rad(rot_deg)
        rot_mat = np.array(
            [[ np.cos(rot_rad), 0, np.sin(rot_rad)],
             [               0, 1,               0],
             [-np.sin(rot_rad), 0, np.cos(rot_rad)]])
        new_position = np.matmul(rot_mat, np.array(frame_info['position'])) * scale + np.array([offset[1], 0, offset[0]])
        new_footprint = np.matmul(rot_mat, np.array(frame_info['footprint'])) * scale + np.array([offset[1], 0, offset[0]])
        new_rotation = np.matmul(np.array(frame_info['rotation']), rot_mat.transpose())  # Because of the definition of OpenMVG rotation!
        return {'position': new_position.tolist(), 'rotation': new_rotation.tolist(), 'footprint': new_footprint.tolist()}
    
   
    def make_geodata(self, timestamp, local_position, babylon_euler_angles, height, gps, world_clock_time) -> list:
        # GPS
        utm_data = utm.from_latlon(gps[0], gps[1])  # The return has the form (EASTING, NORTHING, ZONE_NUMBER, ZONE_LETTER).
        utm_xy = np.array(utm_data[:2])  # utm_xy[0]: east, utm_xy[1]: north
        utm_xy += np.array([local_position[0], local_position[2]])  # local_position[0]: east, local_position[2]: north
        new_gps = utm.to_latlon(utm_xy[0], utm_xy[1], utm_data[2], utm_data[3])
        # Elevation
        elevation = gps[2] - local_position[1]  # local position: y-axis pointing downwards
        # World clock time
        abs_time_ms = world_clock_time + timestamp * 1000

        geodata = [new_gps[1], new_gps[0], elevation, abs_time_ms,  # longitude, latitude, elevation, timestamp
                   height, babylon_euler_angles[0], babylon_euler_angles[1], babylon_euler_angles[2]]
        return geodata

class ConcatConvert2GeojsonGPSAnchored(Task):
    def __init__(self, step: int, prj_dirs: dict, prj_cfg: dict):
        ''' Get config parameters & directories needed, and do some checks. '''
    
        super().__init__(step, prj_dirs['log'])

        self.prj_dir = prj_dirs['prj']
        self.src_dir = prj_cfg['project']['tower_src_dir']

        if ('remake_routes' in prj_cfg['geojson']):
            self.remake_routes = prj_cfg['geojson']['remake_routes']
            if self.remake_routes:
                self.n_lense_for_rot = prj_cfg['project']['n_lense_for_rot']
        else:
            self.remake_routes = False
        
        self.anchor_gps = prj_cfg['geojson']['anchor_gps']
        self.anchor_pts_number = int(prj_cfg['geojson']['anchor_pts_num'])
        self.gps_combined = prj_cfg['geojson']['gps_combined']
        self.smoothen_match = prj_cfg['geojson']['smoothen_match']
        self.smooth_type = prj_cfg['geojson']['smooth_type']
        self.match_footprint = prj_cfg['geojson']['match_footprint']
        self.opt_with_stepsize_anchor = prj_cfg['geojson']['opt_with_stepsize_anchor']
        self.src_mrk_start_sec = prj_cfg['geojson']['src_mrk_start_sec']  
        self.use_cutted_video = prj_cfg['geojson']['use_cutted_video']  
        return
 
    def run_task_content(self) -> int:
        if self.opt_with_stepsize_anchor:
            #generate SfM_path.sfm_path with stepsize anchor
            read_footprint = False
            sfm_path_no_stepsize_obj = self.gen_sfm_path(self.prj_dir)
            gps_path_obj = self.gen_gps_path(read_footprint)
            sfm_path_w_stepsize_anchor = \
                R2G.gen_sfm_path_stepsize_anchor(sfm_path_no_stepsize_obj, gps_path_obj)
            sfm_path = sfm_path_w_stepsize_anchor
        else:
            sfm_path_obj = self.gen_sfm_path(self.prj_dir)
            sfm_path_obj.set_dummy_footprint()
            sfm_path = sfm_path_obj.sfm_path
            read_footprint = True
        
        gps_geojson = self.load_geojson(self.anchor_gps)
        output_path_list  = self.get_output_geojson_path()
        R2G.export_routes_w_GPS_anchor(sfm_path, gps_geojson, 
                                           output_path_list, 
                                           self.anchor_pts_number, 
                                           self.src_mrk_start_sec,
                                           self.use_cutted_video,
                                           self.logger,
                                           self.smoothen_match,
                                           self.smooth_type,
                                           self.match_footprint,
                                           self.gps_combined,
                                           read_footprint = read_footprint)
        return 0

    def gen_sfm_path(self, sfm_base_dir: str) -> R2G.SfM_path:
        sfm_path_obj = R2G.get_sfm_path(sfm_base_dir)
        return sfm_path_obj
    
    def load_geojson(self, anchor_geojson_file: str) -> dict:
        gps_geojson = R2G.load_geojson(anchor_geojson_file)
        return gps_geojson
    
    def gen_gps_path(self, read_footprint):
        gps_geojson = self.load_geojson(self.anchor_gps)
        gps_path_obj = R2G.get_gps_path(gps_geojson, read_footprint=read_footprint)
        gps_path = gps_path_obj.gps_path
        if len(gps_path) < 2:
            raise ValueError('gps path does not have enough points, need at least two')
        elif len(gps_path) <=3:
            gps_path_obj.expand_gps()
        return gps_path_obj

    def get_output_geojson_path(self):
        geojson_path_local = join(self.prj_dir, 'geo_data_sfm.geojson')
        geojson_path_repo = join(self.src_dir, 'geo_data_sfm.geojson')
        return [geojson_path_local, geojson_path_repo]
        
        

